{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LCEL\n",
    "\n",
    "通过对LCEL的各种Primitives组成，可以有多种使用技巧"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 环境变量设置\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-xxx\"\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"https://api.chatanywhere.tech/v1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runnables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用 Runnables来连接多条链"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'她出生于中国的福建省福州市。'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#获取可迭代对象中指定索引或键对应的元素\n",
    "from operator import itemgetter \n",
    "\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt1 = ChatPromptTemplate.from_template(\"{person}出生于哪个城市?\")\n",
    "prompt2 = ChatPromptTemplate.from_template(\n",
    "    \"{city}属于哪个省和哪个城市? 用{language}来回答\"\n",
    ")\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4\")\n",
    "\n",
    "chain1 = prompt1 | model | StrOutputParser()\n",
    "\n",
    "chain2 = (\n",
    "    {\"city\": chain1, \"language\": itemgetter(\"language\")} #获取invoke中的language\n",
    "    | prompt2\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "# chain1.invoke({\"person\": \"庄心妍\"})\n",
    "chain2.invoke({\"person\": \"庄心妍\", \"language\": \"中文\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上面的chain2中，chain1的输出作为city的值继续下一条链"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **多条链高级使用**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "prompt1 = ChatPromptTemplate.from_template(\n",
    "    \"生成一个{attribute}属性的颜色。除了返回这个颜色的名字不要做其他事:\"\n",
    ")\n",
    "prompt2 = ChatPromptTemplate.from_template(\n",
    "    \"什么水果是这个颜色:{color},只返回这个水果的名字不要做其他事情:\"\n",
    ")\n",
    "prompt3 = ChatPromptTemplate.from_template(\n",
    "    \"哪个国家的国旗有这个颜色:{color},只返回这个国家的名字不要做其他事情:\"\n",
    ")\n",
    "prompt4 = ChatPromptTemplate.from_template(\n",
    "    \"有这个颜色的水果是{fruit},有这个颜色的国旗是{country}？\"\n",
    ")\n",
    "\n",
    "\n",
    "model_parser = model | StrOutputParser()\n",
    "# 生成一个颜色\n",
    "color_generator = (\n",
    "    {\"attribute\": RunnablePassthrough()} | prompt1 | {\"color\": model_parser}\n",
    ")\n",
    "color_to_fruit = prompt2 | model_parser\n",
    "color_to_country = prompt3 | model_parser\n",
    "question_generator = (\n",
    "    color_generator | {\"fruit\": color_to_fruit, \"country\": color_to_country} | prompt4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "执行顺序：question_generator =  color_generator | {\"fruit\": color_to_fruit, \"country\": color_to_country} | prompt4\n",
    "\n",
    "- 先根据prompt1生成结果，并将结果作为参数color带给后面的链\n",
    "- color_to_fruit需要color，color_to_country需要的color都在上面传递下来了，执行完成后分别作为参数传递给 fruit 和 country 变量传递给prompt4\n",
    "- prompt4最终接受fruit和country执行链条生成结果\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[HumanMessage(content='有这个颜色的水果是桃子,有这个颜色的国旗是不存在的。？')])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_generator.invoke(\"温柔的\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 多链执行和结果合并 Branching and Merging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RunnableParallels 允许分割或分叉链，以便多个组件可以并行处理输入。\n",
    "\n",
    "最后，其他组件可以加入或合并结果以合成最终响应。这种类型的链创建的计算图形如下:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#      Input\n",
    "#       / \\\n",
    "#      /   \\\n",
    "#  Branch1 Branch2\n",
    "#      \\   /\n",
    "#       \\ /\n",
    "#       Combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "planner = (\n",
    "    ChatPromptTemplate.from_template(\"生成一个关于{input}的论点\")\n",
    "    | ChatOpenAI()\n",
    "    | StrOutputParser()\n",
    "    | {\"base_response\": RunnablePassthrough()} # RunnablePassthrough将上面结果透传下去\n",
    ")\n",
    "\n",
    "arguments_for = (\n",
    "    ChatPromptTemplate.from_template(\n",
    "        \"列举出以下内容的优点和或积极方面{base_response}\"\n",
    "    )\n",
    "    | ChatOpenAI()\n",
    "    | StrOutputParser()\n",
    ")\n",
    "arguments_against = (\n",
    "    ChatPromptTemplate.from_template(\n",
    "        \"列举出以下内容的缺点和或消极方面 {base_response}\"\n",
    "    )\n",
    "    | ChatOpenAI()\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_responder = (\n",
    "    ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"ai\", \"{original_response}\"),\n",
    "            (\"human\", \"积极:\\n{results_1}\\n\\消极:\\n{results_2}\"),\n",
    "            (\"system\", \"根据评论给出最后的回复\"),\n",
    "        ]\n",
    "    )\n",
    "    | ChatOpenAI()\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain = (\n",
    "    planner\n",
    "    | {\n",
    "        \"results_1\": arguments_for,\n",
    "        \"results_2\": arguments_against,\n",
    "        \"original_response\": itemgetter(\"base_response\"),\n",
    "    }\n",
    "    | final_responder\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'比特币作为一种数字货币，的确具有许多积极特性，如去中心化、不可篡改性、有限发行量等，有助于推动金融自由化和国际贸易。然而，也需要认识到比特币存在波动性大、安全性问题、隐私性问题等负面影响。在使用比特币时，需要谨慎考虑这些风险因素，并寻找合适的风险管理策略。比特币的发展和应用仍在不断演变中，我们需要持续关注其发展动态，并做出明智的投资和使用决策。'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"比特币\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RunnableWithMessageHistory 允许我们将消息历史添加到某些类型的链中。它包装另一个 Runnable 并为其管理聊天消息历史记录"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "具体来说，它可用于以下任何输入类型的 Runnable：\n",
    "\n",
    "- 一个 BaseMessage 序列\n",
    "- 一个以序列 BaseMessage 为值的字典\n",
    "- 一个以字符串或序列 BaseMessage 为最新消息的键和以历史消息为值的键的字典\n",
    "\n",
    "\n",
    "并且返回以下任何输出类型之一：\n",
    "\n",
    "- 一个可以作为 AIMessage 内容处理的字符串\n",
    "\n",
    "- 一个 BaseMessage 序列\n",
    "\n",
    "- 一个包含 BaseMessage 序列的字典\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI()\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"你是一个擅长{ability}的助手。回答不超过20个字\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "runnable = prompt | model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用内存存储"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store = {}\n",
    "\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    runnable,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run c5fa54b3-8bdc-4c04-a2a3-8be2a567e8e6 not found for run 2b7022f2-5e94-4772-9678-74a30b66837c. Treating as a root run.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='表示角的余弦值，介于-1到1之间。', response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 45, 'total_tokens': 62}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-a8653b57-7cda-485c-bc54-57a10c196787-0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke(\n",
    "    {\"ability\": \"数学\", \"input\": \"余弦函数是什么意思？\"},\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run cb5c8e82-d6eb-4c17-85f2-714b461e31c0 not found for run 30e95118-ac47-42f1-8c29-69f92fc9c723. Treating as a root run.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='描述角度的余弦值在-1到1之间变化。', response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 74, 'total_tokens': 92}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-a635dd91-55e3-48a2-8795-858fe2a282bb-0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 记住\n",
    "with_message_history.invoke(\n",
    "    {\"ability\": \"数学\", \"input\": \"什么？\"},\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run d7e9f1b1-f866-4136-97dc-ce8f60c278c3 not found for run c7341298-6662-45df-95a3-441fd807663e. Treating as a root run.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='我是一个擅长数学的助手，有什么可以帮到您的吗？', response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 38, 'total_tokens': 65}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-7a2cd72b-bfd8-4f72-a8fc-70349b5b83c2-0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 新的 session_id --> 不记住。\n",
    "with_message_history.invoke(\n",
    "    {\"ability\": \"数学\", \"input\": \"什么？\"},\n",
    "    config={\"configurable\": {\"session_id\": \"def234\"}},\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
